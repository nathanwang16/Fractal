# Fractal

Reproduction and exploration of the paper, + the LessWrong posts, and now, the transformer-lens lib.

1. Reproduction of the chaos games
2. Reproduction of a simplex from a 3-state HMM.
3. compare and contrast
4. exploration of the residual streams in a simple CNN
5. develop the transformer-lens library to adapt to more types of models

Observations:

1. Transformers does more than an observer. Observer is human and thus have pre-conceived knowledge about what will be predicted. Such as only alphabet will be the output, only 3 possible hidden states. Transformer can purely learn these from data.
2. A simplex is a distribution graph of for a HMM where the barycentric values are the corresponding possibilites.
3. 

References:

# Transformers represent belief state geometry in their residual stream

| [https://doi.org/10.48550/arXiv.2405.15943](https://doi.org/10.48550/arXiv.2405.15943) |
| ----------------------------------------------------------------------------------- |

https://www.lesswrong.com/posts/mBw7nc4ipdyeeEpWs/why-would-belief-states-have-a-fractal-structure-and-why

https://www.reddit.com/r/MachineLearning/comments/evsaoa/p_notebook_on_hidden_markov_models_hmms_in_pytorch/
