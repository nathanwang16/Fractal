{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a790766",
   "metadata": {},
   "source": [
    "In the previous notebook we set up a HMM and forward computing process. This one will extend that idea, with a few differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c9482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa7e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"✅ Using Apple M1/M2 GPU via MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"⚠️ MPS not available, falling back to CPU\")\n",
    "# experimenting with torch\n",
    "# unsqueeze function\n",
    "x = torch.tensor([1, 2, 3, 4])\n",
    "c = torch.unsqueeze(x, 0)  # adds a dimension at index 0\n",
    "print(c)\n",
    "b = torch.unsqueeze(x, 1)  # adds a dimension at index 1\n",
    "print(b)\n",
    "\n",
    "       # Hidden State Ha   Hb    Hc\n",
    "transition_matrix = torch.tensor([[0.9,0.05,0.05],\n",
    "                     [0.05,0.9,0.05],\n",
    "                     [0.05,0.05,0.9]])\n",
    "\n",
    "       # emissions  # A   B    C\n",
    "emission_matrix = torch.tensor([[0.9,0.05,0.05],\n",
    "                     [0.05,0.9,0.05],\n",
    "                     [0.05,0.05,0.9]])\n",
    "\n",
    "\n",
    "pi = torch.tensor([0.3,0.4,0.3])\n",
    "\n",
    "eta = torch.tensor([0.9,0.05,0.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5afe92",
   "metadata": {},
   "source": [
    "Now imagine the state and symbols are mixed into a single matrix. Design this matrix M to be able to update once every transition, that transform the current mixed state to a linear combination of it, which is the updated mixed state. Because M already inform us about the probability distribution, the update should include the symbol, and we choose one of the three msp matrix to update the probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e5d403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_msp_matrices(A: torch.Tensor, B: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Given HMM transition matrix A [N, N] and emission matrix B [N, M],\n",
    "    return a list of MSP matrices T_k, one for each observation symbol k.\n",
    "    \"\"\"\n",
    "    N, M = B.shape\n",
    "    T_list = []\n",
    "    \n",
    "    for k in range(M):  # loop over observation symbols\n",
    "        emission_col = B[:, k]                      # shape (N,)\n",
    "        T_k = A * emission_col.unsqueeze(0)         # broadcast multiply to shape (N, N)\n",
    "        T_list.append(T_k)\n",
    "    print(\"T_list:\", T_list)\n",
    "    return T_list  # list of [N x N] tensors, each is T_k\n",
    "\n",
    "def generate_store_token_belief(T_list,pi:torch.tensor,eta,cycle,seed=None):\n",
    "     token = []\n",
    "     belief = []\n",
    "     store = []\n",
    "     eta = eta\n",
    "     print(\"pi:\", pi.shape[0])\n",
    "     if seed is not None:\n",
    "          torch.manual_seed(seed)\n",
    "     for _ in range(cycle):\n",
    "          dice = np.random.choice(pi.shape[0])\n",
    "          #print(\"dice:\", dice)\n",
    "          t_x = T_list[dice]\n",
    "          \n",
    "          eta = eta @ t_x\n",
    "          eta = eta / eta.sum()\n",
    "          \n",
    "          token.append(pi[dice])\n",
    "          belief.append(eta)\n",
    "          store.append(get_cartesian_from_barycentric(eta))\n",
    "     token = torch.tensor(token)\n",
    "     belief = torch.stack(belief)\n",
    "     return token,belief,store\n",
    "\n",
    "# helper function\n",
    "def get_cartesian_from_barycentric(b):\n",
    "    t = np.transpose(np.array([[0,0],[1,0],[0.5, np.sqrt(3)/2]])) # Triangle\n",
    "    return t.dot(b)\n",
    "\n",
    "print(get_cartesian_from_barycentric(eta))\n",
    "\n",
    "def plot_beliefs_on_simplex(beliefs: torch.Tensor, title=\"Belief Trajectory\"):\n",
    "    assert beliefs.shape[1] == 3, \"Only works for 3-state HMM\"\n",
    "\n",
    "    # Make triangle float32 to match beliefs\n",
    "    v0 = torch.tensor([0.0, 0.0], dtype=beliefs.dtype)\n",
    "    v1 = torch.tensor([1.0, 0.0], dtype=beliefs.dtype)\n",
    "    v2 = torch.tensor([0.5, torch.sqrt(torch.tensor(3.0, dtype=beliefs.dtype)) / 2], dtype=beliefs.dtype)\n",
    "    triangle = torch.stack([v0, v1, v2])  # [3, 2]\n",
    "    print(\"triangle:\", triangle)\n",
    "    # Convert belief vectors to xy\n",
    "    xy_coords = beliefs @ triangle  # [T, 2]\n",
    "    print(\"xy_coords:\", xy_coords.shape)\n",
    "    print(\"examples:\", xy_coords[0:50])\n",
    "    # Plot\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(*zip(*torch.cat([triangle, triangle[0].unsqueeze(0)], dim=0)), color='k', lw=1)\n",
    "    plt.scatter(xy_coords[:, 0], xy_coords[:, 1], s=6, c=range(len(beliefs)), cmap='viridis')\n",
    "    plt.title(title)\n",
    "    plt.axis('equal')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb1af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list = compute_msp_matrices(transition_matrix, emission_matrix)\n",
    "tokens, beliefs,store = generate_store_token_belief(t_list, pi=pi,eta=eta, cycle=10000)\n",
    "tokens = tokens.to(device)\n",
    "beliefs = beliefs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbbe58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousHMMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokens, beliefs, seq_len):\n",
    "        self.tokens = tokens\n",
    "        self.beliefs = beliefs\n",
    "        self.seq_len = seq_len\n",
    "        self.length = len(tokens) - seq_len - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.tokens[idx : idx + self.seq_len]           # [seq_len]\n",
    "        y = self.tokens[idx + 1 : idx + self.seq_len + 1]   # [seq_len]\n",
    "        \n",
    "        b = self.beliefs[idx + 1 : idx + self.seq_len + 1]  # [seq_len, 3]\n",
    "        return x, y, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd33900",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=64, n_heads=2, n_layers=2, seq_len=32):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, seq_len, d_model))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=4*d_model,\n",
    "            activation=\"relu\",\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, return_hidden=False):\n",
    "        \"\"\"\n",
    "        x: [B, T] token indices\n",
    "        returns:\n",
    "            logits: [B, T, vocab_size]\n",
    "            hidden: [B, T, d_model] if return_hidden=True\n",
    "        \"\"\"\n",
    "        tok = self.token_emb(x)                   # [B, T, D]\n",
    "        x = tok + self.pos_emb[:, :x.size(1), :]  # add positional encoding\n",
    "        hidden = self.transformer(x)              # [B, T, D]\n",
    "        logits = self.output(hidden)              # [B, T, vocab]\n",
    "\n",
    "        if return_hidden:\n",
    "            return logits, hidden\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e901a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, epochs=10, lr=1e-3, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, y, _ in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch}: Loss {total_loss:.4f}\")\n",
    "        \n",
    "def extract_residuals(model, dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    all_hidden = []\n",
    "    all_beliefs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, _, beliefs in dataloader:\n",
    "            x = x.to(device)\n",
    "            logits, hidden = model(x, return_hidden=True)\n",
    "            all_hidden.append(hidden.cpu())\n",
    "            all_beliefs.append(beliefs)\n",
    "\n",
    "    reps = torch.cat(all_hidden, dim=0).reshape(-1, hidden.shape[-1])\n",
    "    beliefs = torch.cat(all_beliefs, dim=0).reshape(-1, beliefs.shape[-1])\n",
    "    return reps, beliefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14deb6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokens.long()\n",
    "tokens.to(device)\n",
    "model = SimpleTransformer(vocab_size=3, d_model=64, n_heads=2, n_layers=2, seq_len=32)\n",
    "model.to(device)\n",
    "ds = ContinuousHMMDataset(tokens, beliefs, seq_len=32)\n",
    "dataloader = torch.utils.data.DataLoader(ds, batch_size=16, shuffle=False, drop_last=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "train_model(model, dataloader, epochs=4, device=device)\n",
    "\n",
    "reps, belief_targets = extract_residuals(model, dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = reps.detach().cpu().numpy()\n",
    "Y = belief_targets.detach().cpu().numpy()\n",
    "\n",
    "probe = LinearRegression()\n",
    "probe.fit(X, Y)\n",
    "\n",
    "Y_pred = probe.predict(X)\n",
    "pred_tensor = torch.tensor(Y_pred, dtype=torch.float32)\n",
    "plot_beliefs_on_simplex(pred_tensor, title=\"Transformer Residual → Belief Geometry\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
